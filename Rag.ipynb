{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response: Question\n",
      ": In case UMB is being offered to the animal for licking, how much quantity of urea treated straw should \n",
      "be fed to animal?\n",
      "Answer: UMB and urea treated straw both should not be fed to animal at a time. If UMB is not given, we can offer \n",
      "urea treated straw to animal to the extent it eats. Question\n",
      ": Do we need to provide extra mineral mixture when we are feeding balanced cattle feed to animals?\n",
      "Answer: As balanced cattle feed contains mineral mixture, we can reduce the recommended quantity of minerium to a minimum. Question\n",
      "\n",
      ": What is the amount of urea treated straw that should be fed to animals?\n",
      "Answer: Urea treated straw should\n",
      "Generated Text: In case UMB is being offered to the animal for licking, how much quantity of urea treated straw should be fed to animal?\n",
      "\n",
      "Urine is not a good source of urea. It is not a good source of urea.\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "import warnings\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        num_pages = len(pdf_reader.pages)\n",
    "        for page_num in range(num_pages):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "            text += \"\\n\\n\"  # Add two newline characters to separate pages\n",
    "\n",
    "    # Add delimiter before each section title\n",
    "    text = text.replace(\"\\n\\n\", \"\\n     \\n\\n\")  # Replace double newline characters with a delimiter\n",
    "\n",
    "    # Add new paragraph after the text \"Question\"\n",
    "    text = text.replace(\"Question\", \"\\nQuestion\\n\")\n",
    "\n",
    "    return text\n",
    "\n",
    "def segment_text(text):\n",
    "    # Improved pattern to match various newline combinations\n",
    "    pattern = r\"\\n{2,}\"  # Matches two or more consecutive newlines\n",
    "    paragraphs = re.split(pattern, text)\n",
    "    return paragraphs\n",
    "\n",
    "def embed_sentence(sentence):\n",
    "    \"\"\"Embeds a sentence into a vector using the sentence transformer model\"\"\"\n",
    "    sentence_embeddings = model.encode(sentences=[sentence])\n",
    "    return sentence_embeddings[0]  # Extract the first embedding (for single sentence)\n",
    "\n",
    "def search_with_similarity(query, text):\n",
    "    \"\"\"\n",
    "    Searches for relevant passages based on contextual similarity using sentence transformers.\n",
    "\n",
    "    Args:\n",
    "        query: The user's query as a string.\n",
    "        text: The segmented text from the PDF document (list of paragraphs).\n",
    "\n",
    "    Returns:\n",
    "        A list of the top K most similar passages to the query.\n",
    "    \"\"\"\n",
    "    passages = []\n",
    "    query_embedding = embed_sentence(query)\n",
    "\n",
    "    for paragraph in text:\n",
    "        passage_embedding = embed_sentence(paragraph)\n",
    "        similarity_score = util.cos_sim(query_embedding, passage_embedding)\n",
    "        passages.append((paragraph, similarity_score))\n",
    "\n",
    "    # Sort passages by similarity score in descending order (most similar first)\n",
    "    passages.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Return only the top K passages\n",
    "    return passages[:2]\n",
    "\n",
    "def generate_text(prompt, max_length=50, temperature=0.7, top_k=50, top_p=0.95, num_return_sequences=1):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    output = model.generate(input_ids, max_length=max_length, temperature=temperature, top_k=top_k, top_p=top_p, num_return_sequences=num_return_sequences)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "def generate_response(passages, max_length=150, temperature=0.7, top_k=50, top_p=0.95, num_return_sequences=1):\n",
    "    combined_passages = \" \".join([passage[0] for passage in passages])\n",
    "    input_text = combined_passages[:512]  # Limit input length for GPT-2 model\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids, attention_mask=attention_mask, max_length=max_length, \n",
    "                                temperature=temperature, top_k=top_k, top_p=top_p, \n",
    "                                num_return_sequences=num_return_sequences, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Path to the PDF document\n",
    "pdf_path = r\"C:\\Users\\chaitanya\\Downloads\\Knowledge base for RAG-Handbook-of-Good-Dairy-Husbandry-Practices_.pdf\"\n",
    "\n",
    "# Extract text from the PDF document\n",
    "pdf_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Segment text into paragraphs\n",
    "text = segment_text(pdf_text)\n",
    "\n",
    "# Load the pre-trained sentence transformer model\n",
    "model = SentenceTransformer('all-mpnet-base-v2')  # Replace with your desired model\n",
    "# Example usage (assuming you have segmented text 'text')\n",
    "query = \"In case UMB is being offered to the animal for licking, how much quantity of urea treated straw should be fed to animal?\"\n",
    "relevant_passages = search_with_similarity(query, text)\n",
    "# Load pre-trained GPT model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "\n",
    "\n",
    "# Generate text based on retrieved passages\n",
    "generated_response = generate_response(relevant_passages)\n",
    "\n",
    "# Print generated response\n",
    "print(\"Generated Response:\", generated_response)\n",
    "\n",
    "# Generate text from prompt\n",
    "generated_text = generate_text(query)\n",
    "print(\"Generated Text:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
